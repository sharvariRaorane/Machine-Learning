{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ML practical 10 reinforcement_learning .ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyN2oLESboeQr6krBZowWG17"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Installing dependencies"],"metadata":{"id":"peVOMWT48ezV"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9cye6XY38chh","executionInfo":{"status":"ok","timestamp":1649879959737,"user_tz":-330,"elapsed":6837,"user":{"displayName":"16.Sharvari Raorane","userId":"08242520697668490314"}},"outputId":"f62f2b70-b17d-4f1e-f069-d6b3f4a606b0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.21.5)\n","Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (0.17.3)\n","Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.3.0)\n","Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.5.0)\n","Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym) (1.21.5)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym) (1.4.1)\n","Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.16.0)\n"]}],"source":["!pip install numpy\n","!pip install gym"]},{"cell_type":"markdown","source":["Importing necessary libraries"],"metadata":{"id":"BdpJDDk09BEH"}},{"cell_type":"code","source":["import numpy as np\n","import gym\n","import random"],"metadata":{"id":"COkXmnk-8ynq","executionInfo":{"status":"ok","timestamp":1649880012032,"user_tz":-330,"elapsed":499,"user":{"displayName":"16.Sharvari Raorane","userId":"08242520697668490314"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["Creating the environment"],"metadata":{"id":"BQKFt5qS9VWN"}},{"cell_type":"code","source":["env = gym.make(\"FrozenLake-v0\")"],"metadata":{"id":"aw_0NUUt9V5C","executionInfo":{"status":"ok","timestamp":1649880061112,"user_tz":-330,"elapsed":1026,"user":{"displayName":"16.Sharvari Raorane","userId":"08242520697668490314"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["Initiating Q table"],"metadata":{"id":"AKvZwBLK9noa"}},{"cell_type":"code","source":["action_size = env.action_space.n\n","state_size = env.observation_space.n"],"metadata":{"id":"apzhDxLt9iWz","executionInfo":{"status":"ok","timestamp":1649880103167,"user_tz":-330,"elapsed":438,"user":{"displayName":"16.Sharvari Raorane","userId":"08242520697668490314"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# create our Q table with state_size rows and action_size column(64x4)\n","qtable = np.zeros((state_size, action_size))\n","print(qtable)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eYNI-Gcw-Dcu","executionInfo":{"status":"ok","timestamp":1649880105937,"user_tz":-330,"elapsed":418,"user":{"displayName":"16.Sharvari Raorane","userId":"08242520697668490314"}},"outputId":"9c04b110-d275-44f6-b992-9871717e41fb"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["[[0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]]\n"]}]},{"cell_type":"markdown","source":["Creating hyper-parameters"],"metadata":{"id":"2zPMEsf_GHx-"}},{"cell_type":"code","source":["total_episodes = 20000 #Total episodes\n","learning_rate = 0.7 #Learning rate\n","max_steps = 99 #Max steps per episode\n","gamma = 0.95 #Discounting rate\n","\n","#Exploration parameters\n","epsilon = 1.0 #Exploration rate\n","max_epsilon = 1.0 #Exploration probability at start\n","min_epsilon = 0.01 #Minimum exploration probability\n","decay_rate = 0.005 #Exponential decay rate for exploration prob"],"metadata":{"id":"PumGD3GFGFPk","executionInfo":{"status":"ok","timestamp":1649880139929,"user_tz":-330,"elapsed":413,"user":{"displayName":"16.Sharvari Raorane","userId":"08242520697668490314"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["Q-Learning algorithm"],"metadata":{"id":"eTEtEoRLHdN-"}},{"cell_type":"code","source":["#List of rewards \n","rewards = []\n","\n","#2 for life or until learning is stopped\n","for episode in range(total_episodes):\n","  # Reset the environment\n","  state = env.reset()\n","  step = 0\n","  done = False\n","  total_rewards = 0\n","  \n","  for step in range(max_steps):\n","    #3. Choose an action a in the current world state (s)\n","    ## First we randonize a number\n","    exp_exp_tradeoff = random.uniform(0, 1)\n","\n","    ## if this number > greater than epsilon --> exploitation (taking the biggest Q value for this state)\n","    if exp_exp_tradeoff > epsilon:\n","      action = np.argmax(qtable[state, :])\n","      #print(exp_exp_tradeot, \"action\", action)\n","\n","    # Else doing o randon cholce --> exploration\n","    else:\n","      action = env.action_space.sample()\n","      #print(\"action randon\", action)\n","\n","    # Take the action (a) and observe the outcome state(s') and reward (r)\n","    new_state, reward, done, info = env.step(action)\n","    \n","    # Update Q(s,a):= Q(s,a) + Ir [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n","    # qtable[new_state, :] : all the actions we can take from new state\n","    qtable[state, action] = qtable[state, action] + learning_rate * (reward + gamma * np.max(qtable[new_state, :]) - qtable[state, action])\n","    \n","    total_rewards += reward\n","    \n","    # our new state is state\n","    state = new_state\n","    \n","    # if done (1F we're dead) : finish episode\n","    if done == True:\n","      break\n","    \n","    # Reduce epsilon (because we need less and less exploration) \n","    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode)\n","    rewards.append(total_rewards) \n","\n","print (\"Score over time: \" + str(sum(rewards)/total_episodes))\n","print(qtable) \n","      \n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pyIv255sHd8v","executionInfo":{"status":"ok","timestamp":1649880329206,"user_tz":-330,"elapsed":22779,"user":{"displayName":"16.Sharvari Raorane","userId":"08242520697668490314"}},"outputId":"46c440a1-de43-46cb-e390-f955501054bf"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Score over time: 0.0\n","[[2.59279733e-01 8.04156935e-02 5.99814719e-02 7.40982763e-02]\n"," [4.90424888e-03 2.75377439e-03 4.02054285e-03 1.26455676e-01]\n"," [3.66005916e-02 5.14043257e-03 4.24936124e-03 5.70128363e-03]\n"," [4.86536754e-03 2.65784154e-03 3.47465990e-03 5.64553223e-03]\n"," [2.66966749e-01 2.76411591e-02 3.94492604e-02 9.58064885e-05]\n"," [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n"," [2.29378672e-04 3.60892121e-05 1.44959663e-01 2.47248178e-05]\n"," [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n"," [4.56318288e-02 1.06548618e-01 9.61038065e-02 3.68840321e-01]\n"," [4.82132273e-02 4.39660718e-01 1.72248494e-02 1.94575708e-02]\n"," [7.77430377e-01 1.26066064e-01 1.81698802e-02 7.19659713e-05]\n"," [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n"," [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n"," [1.40677970e-01 2.14671305e-02 6.45848430e-01 1.36547490e-01]\n"," [1.54882829e-01 8.92863718e-01 4.21762021e-01 3.88792123e-01]\n"," [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n"]}]},{"cell_type":"markdown","source":["Using Q table to play now"],"metadata":{"id":"81XZpyMBNwJ1"}},{"cell_type":"code","source":["env.reset()\n","\n","for episode in range(5):\n","  state = env.reset()\n","  step = 0\n","  done = False\n","  print(\"********************************\")\n","  print(\"EPISODE \", episode)\n","  \n","  for step in range(max_steps):\n","    \n","    # Take the action (index) that have the maximum expected future reward given that state\n","    action = np.argmax(qtable[state, :])\n","    \n","    new_state, reward, done, info = env.step(action)\n","    \n","    if done:\n","      # Here, we decide to only print the last state (to see if our agent is on the goal or fall into an hole)\n","      env.render()\n","      if new_state == 15:\n","        print(\"We reached our Goal \")\n","      else:\n","        print(\"We fell into a hole \")\n","      \n","      # We print the number of step it took.\n","      print(\"Number of steps\", step)\n","      \n","      break\n","    state = new_state\n","env.close()\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"F7RE6iN-Lh8G","executionInfo":{"status":"ok","timestamp":1649880392655,"user_tz":-330,"elapsed":520,"user":{"displayName":"16.Sharvari Raorane","userId":"08242520697668490314"}},"outputId":"e87706ff-e4c8-40e1-d4a6-9013c38a43a6"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["********************************\n","EPISODE  0\n","  (Right)\n","SFFF\n","FHF\u001b[41mH\u001b[0m\n","FFFH\n","HFFG\n","We fell into a hole \n","Number of steps 29\n","********************************\n","EPISODE  1\n","  (Down)\n","SFFF\n","FHFH\n","FFFH\n","HFF\u001b[41mG\u001b[0m\n","We reached our Goal \n","Number of steps 6\n","********************************\n","EPISODE  2\n","  (Down)\n","SFFF\n","FHFH\n","FFFH\n","HFF\u001b[41mG\u001b[0m\n","We reached our Goal \n","Number of steps 33\n","********************************\n","EPISODE  3\n","  (Right)\n","SFFF\n","FHF\u001b[41mH\u001b[0m\n","FFFH\n","HFFG\n","We fell into a hole \n","Number of steps 65\n","********************************\n","EPISODE  4\n","  (Down)\n","SFFF\n","FHFH\n","FFFH\n","HFF\u001b[41mG\u001b[0m\n","We reached our Goal \n","Number of steps 34\n"]}]}]}